---
title: 大数据挖掘与数据分析笔记
date: 2024-05-13 11:46:14
tags: 大数据
categories: 笔记
keywords: 大数据
comments: true
---
# 绪论

## 数据挖掘的由来

**背景**

- 数据爆炸但知识却反
- 从商业数据到商业智能的进化
  - 数据搜集
  - 数据访问
  - 数据仓库,决策支持
  - 数据挖掘
- 科学发展范式
  - 经验科学
  - 理论科学
  - 计算科学
  - 数据科学

**定义**

数据挖掘是从{% label 大量的,不完全的,有噪声的,模糊的,随机的 orange %}{% label 数据 red %}中提取{% label 隐含在其中的,人们事先不知道的,但又是潜在有用的 orange %}{% label 信息和知识 red %}的过程

## 基本概念

**数据**

"8000","1000"是数据

**信息**

"8000米是飞机飞行最大高度"与"1000米的高山是信息"

**知识**

"飞机无法飞越这座高山是知识

**智慧**

"飞机必须飞得比山高"是智慧

## 主要内容

**关联规则挖掘**

牛奶和尿布

**非监督式机器学习--聚类**

**监督式机器学习**

- **离散标签预测-标签分类**

  - 学习建模

  - 分类测试


- **连续标签预测-数值预测**

**回归**

# 认识数据

## 数据类型

**数据对象**

- 数据集由数据对象组成
- 一个数据对象代表一个实体
- 例子
  - 销售数据库： 客户，商店物品，销售额
  - 医疗数据库： 患者，治疗信息
  - 大学数据库： 学生，教授，课程信息
- 称为样品，实例，实例，数据点，对象，元组（tuple）
- 数据对象所描述的属性
  - 数据库中的行：数据对象
  - 列： 属性

**属性**

- 属性（变量，特性，字段，特征或维）： 一个数据字段，代表一个数据对象的特征或功能
  - 例如： 乘客_ID  是否存活   客舱等级
- 类型：
  - 标称:类别,状态
    - 颜色
    - 婚姻状况
    - 职业
    - 身份证号码
  - 二进制
    - 对称二进制
    - 非对称二进制
  - 序数:有一个有意义的顺序,排名,但不知道连续值之间的大小
  - 区间标度
    - 以单位长度顺序性度量
    - 值有序,比如温度,日历等
    - 不存在0点,倍数没有意义:{% label 我们平常不说2024年是1012年的2倍 blue%}
  - 比率标度
    - 具有固定零点的数值属性
    - 有序且可以计算倍数
    - 如长度,重量等

## 数据统计汇总

- 箱线图：分析多个属性数据的离散度差异性
- 直方图：单个属性在各个区间变化分布
- 散点图：两组数据相关性分布

## 数据相似性和相异性度量

**数据矩阵**：N个数据，p个维度

**相异矩阵**：N个数据点，记录两点之间的距离，是一个下三角矩阵

**相似度**：

- 度量两个数据对象有多相似
- 值越大就表示数据对象越相似
- 取值范围通常为[0,1]

**相异度**：

- 度量两个数据对象的差别程度
- 值越小就表示数据越相似
- 最小相异度通常为0

**邻近性**：

- 指相似度或者相异度

**标称属性可以去两个或多个状态**

**方法**：简单匹配

- m：匹配次数；p：属性总数

$d(i,j)=\frac{p-m}{p}$

**二值属性**

**方法**:使用邻接表

|      | 1    | 0    | sum  |
| ---- | ---- | ---- | ---- |
| 1    | q    | r    | q+r  |
| 0    | s    | t    | s+t  |
| sum  | q+s  | r+t  | p    |

**杰卡德相似系数**：距离度量非对称的二值变量

$sim_{jaccard}(i,j)=\frac{q}{q+r+s}$

$d(i,j)=\sqrt[h]{\abs{x_{i1}-x_{j1}}+\abs{x_{i2}-x_{j2}}^h+...+\abs{x_{ip}-x_{jp}}^h}$

- h=1:曼哈顿距离
- h=2:欧氏距离
- h=3:"上确界"距离

# 数据预处理

## 数据清洗

- 数据缺失
  - 删除整个元组(当类标号缺少时)
  - 使用 {% label 使用属性的平均值填充空缺值 blue%}
- 含有噪音,错误,或离群
  - 删除整个元组
- 不一致的代码或不符的名称
  - 替换规则修正

## 数据集成

- 模式集成
  - 整合来自不同来源的数据
- 实体识别问题
- 数据冲突检测

**冗余信息处理**

**方法**:相关分析

- 离散变量
  - 卡方检验
    - $\mathcal{X}^2=\Sigma \frac{(Observed-Expected)^2}{Expected}$
    - 行合计乘以列合计除以总数
- 连续变量
  - 相关系数(也成为{% label 皮尔逊相关系数 orange%}
    - $r_{p,q}=\frac{\Sigma{(p-\overset{-}{p})(q-\overset{-}{q})}}{(n-1)\sigma_p \sigma_q}=\frac{\Sigma(pq)-n\overset{-}{p}\overset{-}{q}}{(n-1)\sigma_p\sigma_q}$
- 协方差
  - $Cov(p,q)=E(p-\overset{-}{p})(q-\overset{-}{q})=\frac{\Sigma_{i=1}^n(p_i-\overset{-}{p})(q_i-\overset{-}{q})}{n}=E(pq)-E(p)E(q)$
  - $r_{p,q}=\frac{Cov(p,q)}{\sigma_p\sigma_q}$
  - 

## 数据规约

**数据降维**

- 原因:
  - 随着维数的增加,数据变得越来越{% label 稀疏 pink%}
  - 子空间的可能的{% label 组合将成倍增长 orange%}
  - 机器学习模型过于复杂
- 方法:
  - PCA主成分分析法
    - 设法将原来众多{% label 具有一定相关性的属性 purple %}(比如p个属性),重新组合成一组{% label 相互无关的综合属性 green%}来代替原来属性.通常是将原来的属性{% label 线性组合 pink %}作为新的属性

**降数据**

- 简单随机抽样

**数据压缩**

## 数据转换

**规范化**:按比例缩放至一个具体的区间

- 最小-最大规范化
  - $v^{'}=\frac{v-min_A}{max_A-min_A}(new\_max_A-new\_min_A)+new\_min_A$
- Z-得分正常化(标准化):通常用于同分布流式数据(不断有新的数据进入数据仓库)
  - $v^{'}=\frac{v-均值A}{标准差A}$
- 小数定标规范化:移动属性A的小数点位置(移动位数依赖于{% label A的最大值 blue%})

**离散化**:

**方法**

- 等宽法:根据属性的{% label 值域 orange%}来划分,使每个区间的宽度相等
- 等频法:根据取值出现的频数来划分,将属性的值域划分成若干个小区间,满足{% label 落在每个区间的样本数目相等 blue%}

**聚类**



# 朴素贝叶斯分类

## 基本概念

**分类**:找出描述和区分数据类或概念的{% label 模型 blue%},以便能够使用模型{% label 预测类标号未知对象的类标号 orange%}

**一般过程**:

- 学习阶段
  - 建立描述预先定义的数据类或概念集的分类器
  - 训练集提供了每个训练元组的类标号,分类的学习过程也称为监督许欸小
- 分类阶段
  - 使用定义好的分类器进行分类的过程

**概念区分**

- 分类与预测
  - 分类是预测分类(离散,无序)标号
  - 预测建立连续值函数模型
- 分类与聚类
  - 分类是有监督学习,提供了训练元组的类标号
  - 聚类是无监督学习,不依赖有类标号的训练实例

**朴素贝叶斯分类**

$P(h_1|D)=\frac{P(D|h_1)P(h_1)}{P(D|h_1)P(h_1)+P(D_2|h_2)P(h_2)}$

- h:假设类别
- D:待测试数据

**极大后验假设(MAP)**

学习器在候选假设集合H中寻找{% label 给定数据D时可能性最大的假设h blue %}被称为极大后验假设

**连续数据**

- 转化为离散
- 正态分布概率密度函数

**特点**

- 属性可以离散,可以连续
- 数学基础扎实,分类效率稳定
- 对缺失和噪声数据不太敏感
- 属性如果不相关,分类效果很好

# 决策树分类

## 决策树的基本概念

**测试结点**:

表示某种作为判断条件的属性

**分支**:

根据条件属性取值选取的路径

**叶子**:

使判断终止的结论

## 决策树的构建方法

**Hunt算法**

$设D_t是与节点t相关联的训练记录集$

**算法步骤:**

- 如果$D_t$中所有记录都属于同一个类$y_t$,则t是叶节点,用$y_t$标记
- 如果$D_t$中包含属于多个类的记录,则{% label 选择一个属性测试条件 orange%},将记录划分成较小的子集
- 对于测试条件的每个输出,创建一个子结点,并根据测试结果将$D_t$中的记录分布到子节点中.然后,对于每个子节点,递归地调用该算法

**Hunt算法采用贪心策略构建决策树,在选择划分数据的属性时,采取一系列局部最优决策来构造决策树.**

**决策树归纳的设计问题**

- 如何分裂训练记录
- 怎样为不同类型的属性指定测试条件
- 怎样评估每种测试条件
- 如何停止分裂过程

**划分方法**

- 二元化分
- 多路划分:注意{% label 序数类数据在多路划分时要保持顺序 green %}

**纯性**

选择最佳划分的度量通常是根据划分后子节点纯性的程度

纯性的程度越高，类分布就越倾斜，划分结果越好。

- C0:5,C1:5，纯性小，不确定性大
- C0:1,C1:9, 纯性大，不确定性小

**信息熵**

$Entropy(t)=-\Sigma_j p(j|t)log[p(j|t)]$

**信息增益算法,ID3算法**

有限选取信息熵减少最大的分类标准

**剪枝**:防止过拟合

- 事先剪枝:提前设置阈值
- 事后剪枝:利用测试集验证来剪枝

**特点**

- 决策树是一种构建分类模型的非参数方法
- 不需要昂贵的计算代价
- 决策树相对容易解释
- 决策树是学习离散值函数的典型代表
- 决策树对噪声的干扰具有相当好的鲁棒性
- 冗余属性不会对决策树的准确率造成不利影响
- 决策树无法学习特征之间的线性关系:{% label 特征构造 orange%}

# K-Means聚类

## 划分的方法

**划分方法**

将有n个对象的数据集D划分成k个簇,并且$k\le n$

满足如下的要求:

- 每个簇至少包含一个对象
- 每个对象属于且仅属于一个簇

**基本思想**

- 首先创建一个初始k划分(k为要构造的划分数)
- 然后不断迭代地计算各个簇的聚类中心,并依据新的聚类中心调整聚类情况,直至收敛.

**目标**

- 同一个簇的对象尽可能紧凑
- 不同簇的对象尽可能不同

**方法**:

- 启发式方法
  - $E=\Sigma_{i=1}^k \Sigma_{p \in C_i}(d(p,c))^2$
  - k-均值(k-means)
    - 每个簇用该簇中对象的均值来表示
    - 基于质心的技术
    - 收敛条件:均值点基本不变化,数据调整很小
    - 优点:
      - 聚类时间快
      - 当结果簇是密集的,而簇与簇之间区别明显时,效果较好
      - 相对可扩展和有效,能对大数据集进行高校划分
    - 缺点:
      - 用户必须事先指定聚类簇的个数
      - 常常终止于局部最优解
      - 只适用于数值属性聚类{% label 计算均值有意义 blue %}
      - 对噪声和异常数据很敏感
      - 不同的初始值,结果可能不同
      - 不适合发现非凸面形状的簇
  - KMeans++算法 -- 解决初始点选择问题
    - 基本原理
      - 从输入的数据点集合中随机选择一个点作为第一个聚类中心
      - 对于数据集中的每一个点{% label X orange%},计算其与聚类中心的距离{% label D(X) orange%}
      - 重复2和3步直到K选择一个D(X)最大的点作为新的聚类中心
      - 各聚类中心被选出
      - 利用K个初始聚类中心运行KMeans
  - k-中心点(k-medoids)
    - 每个簇用接近簇中心的一个对象来表示
    - 基于代表对象的技术

**适用性**

- 适合发现中小规模的数据库中的球状聚类
- 对于大规模数据库和处理任意形状的聚类,这些算法需要进一步扩展.

## 层次的方法

## 基于密度的方法

**DBSCAN算法**

# 逻辑回归

## 逻辑回归基本原理

**logistic函数**

$f(x)=\frac{e^x}{1+e^x}$

$p_i=P(y_i=1|x_{i1},x_{i2},...,x_{ip})$

取对数得:

$Logit(p_i)=ln\frac{p_i}{1-p_i}=\alpha+\beta_1 x_{i1}+\beta_2 x_{i2}+...+\beta_p x_{ip}$

**优势比OR(odds ratio)**

- 流行病学衡量危险因素作用大小的{% label 比数比例 orange%}指标.
- 计算公式为:

$OR_j=\frac{ \frac{P_1}{1-P_1}}{ \frac{P_0}{1-P_0}}$

## 逻辑回归正则化

**权值系数w希望越小越好**

**正则化表达式**

- $L_1=\Sigma \vert{w_i}\vert$,倾向于使w要么取1,要么取0,即{% label 稀疏编码 orange%}
- $L_2=\Sigma w_i^2$,倾向于使得w整体偏小,即{% label 岭回归 pink%}

# 关联规则挖掘

## 关联规则挖掘的概念与基本算法

**关联分析**

用于发现隐藏在大型数据集中令人感兴趣的联系,所发现的模式通常用{% label 关联规则或频繁项集 orange%}的形式表示

**项集(Itemset)**

- 包含0个或多个项的集合

- 例子{Milk,Bread,Diaper}

- k-项集:一个项集包含k个项

**支持度计数(Support count)($\sigma$)**

- 包含{% label 特定项集的事务个数 blue%}

**支持度(Support)**

- 包含项集的事务数与总事务数的比值

**频繁项集(Frequent Itemset)**

- 满足最小支持度阈值(minsup)的所有项集.

**最大频繁项集(Maximal Frequent Itemset)**

- 最大频繁项集的{% label 直接超集 orange %}都不是频繁的

**关联规则的强度**

- 支持度Support(s)
  - 确定项集的频繁程度
- 置信度Confidence(c)
  - 确定Y在包含X的事务中出现的频繁程度

**步骤**

- 频繁项集的产生(Frequent Itemset Generation) 
  - 其目标是发现满足最小支持度阈值的所有项集,这些项集称作频繁项集
- 规则的产生(Rule Generation) 
  - 其目标是从上一步发现的频繁项集中提取所有高置信度的规则,这些规则被称作{% label 强规则 blue%}

## Apriori算法

**先验原理**

- 如果一个项集是频繁的,则它的所有{% label 子集 red%}一定也是频繁的
- 相反,如果一个项集是非频繁的,则它的所{% label 超集 red%}也一定是非频繁的

>例如
>
>如果AB,不是频繁项集,那ABC,ABD,ABCD等肯定都不是频繁项集,直接剪枝,无需计算

**特点**

- 多次扫描数据库
- 候选项规模庞大
- 计算支持度开销大

**改进方法**

- 散列项集计数
- 事务压缩
- 划分
- 采样

## FPGrowth算法

**基本思想**

- 只扫描数据库两遍,构造频繁模式树(FP-Tree)
- 自底向上递归产生频繁项集
- FP树是一种输入数据的压缩表示,它通过逐个读入事务,并把{% label 每个事务映射到FP树中的一条路径来构造 orange %}

**步骤**

- 扫描数据库一次,得到频繁1-项集
- 把项按支持度递减排序
- 再一次扫描数据库,建立FP-tree

**特点**

- 完备
  - 不会打破交易中的任何模式
  - 包含了频繁模式挖掘所需的全部信息
- 紧密
  - 支持度降序排列：支持度高的项在FP-tree中共享的机会也高
  - 绝不会比原数据库大.

## 规则挖掘



# 支持向量机

## 支持向量机的数学原理

## 支持向量机扩展

# 神经网络分类

## 神经网络基础概念

## 神经网络数学原理

# 集成学习

## Bagging分类

## 随机森林分类

## Boosting分类基础

## Boosting分类理论

## GBDT学习

