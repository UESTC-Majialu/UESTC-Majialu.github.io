---
title: 大数据挖掘与数据分析笔记
date: 2024-05-11 21:46:12
description: 这是大数据挖掘与分析的期末整理复习笔记
tags:
  -期末
  -复习
  -数据分析
---

# 一、什么是数据挖掘

数据挖掘就是从数据中隐含的知识发现商业价值

# 二、为什么要数据挖掘

- 海量数据
- 维度众多
- 问题复杂

# 三、数据挖掘有什么用

## (一)分类问题

## (二) 聚类问题

## (三) 回归问题

## (四) 关联问题

# 四、怎么数据挖掘

## (一) 业务理解和数据理解(Business Understanding and Data Understanding)

理解你的数据挖掘要解决什么业务问题

必须从==商业或者业务==角度去了解项目的要求和最终的目的

- 资源
- 局限
- 设想
- 风险
- 意外

### 1. 业务背景与目标

### 2. 把握数据

#### (1) 是否有数据

是否有这样一个数据集来支持你做这样一个模型，来完成这样一个需求，来回答业务问题

#### (2) 有多少数据

数量的不同会影响处理方式

#### (3) 是什么样的数据

考虑这些维度是否可以支持完成业务需求，是否与所提出的问题有关系

#### (4) 标签

比如监督学习任务，每条数据都需要有结果的标注，这也是模型或者算法要学习的结果

## (二) 数据理解(Data Understanding)

在业务理解的基础上，对掌握的数据要有一个清晰、明确的认识

## (三) 数据准备 (Data Preparation)

**数据准备**是基于原始数据，去构建数据挖掘所需的数据集的所有工作

- 数据收集
- 数据清晰
- 数据补全
- 数据整合
- 数据转换
- 特征提取

### 1. 准备数据

如何处理出完整，干净的数据

### 2. 找到数据

每个项目需要什么数据，并从哪里获取

- 关系型数据库：MySQL
- 大数据Hbase,HIVE
- 搜索引擎数据库ES
- 内存数据库Redis
- 图数据库NEO4j,JanusGraph

### 3. 数据探索

把数据变多，升维

### 4. 数据清洗

#### (1) 缺失值的处理

一条新闻可能只有正文没有标题，发布地点，发布时间等任意数据

分析数据缺失的原因以及数据缺失的影响范围

$缺失值的处理\begin{cases}删掉有缺失值的数据\\补充缺失值\\不做处理\end{cases}$

#### (2) 异常值的处理

**异常值:**与样本空间中绝大多数数据分布差距过大的数据

- 错误的情况:比如医院录入病人病历的时候,忘了给数字输入小数点
- 正常的情况:在平均充值为100元的游戏中,有人充了100万元

#### (3) 数据偏差的处理

- 数据本身的错误:对数据进行修正,或者直接丢弃
- 数据是正确的:根据业务需求进行处理

数据偏差可能导致后面训练的模型过拟合或者欠拟合

#### (4) 数据标准化

#### (5) 特征选择

尽可能留下==较少的==数据维度

## (四) 构建模型(Modeling)

**构建模型**也叫做训练模型，重点解决技术方面的问题

选用各种各样的算法模型来处理数据，让模型学习数据的规律，并产出模型

### 1. 构建训练集与测试集

- 留出法:直接把整个数据集划分为两个互斥的部分,使得训练集和测试集互不干扰
- 交叉验证法:先把数据集划分成n个小的数据集,每次使用n-1个数据集作为训练集,剩下的作为测试集进行n次训练
- 自助法: 通过重复抽样构建数据集,通常在小数据集的情况下非常适用

### 2. 数据建模:该如何选择一个适合需求的算法

### 3. 分类问题

==分类是有监督的学习过程==

首先要有一批已经有标签结果的数据

常见算法:

- 最近邻算法(K-NearrestNeighbor,KNN)
  - 原理:找到K个与新数据最近的样本,取样本中最多的一个类别作为新数据的类别
  - 优点:
    - 简单易实现
    - 对于边界不规则的数据效果好
  - 缺点:
    - 只适合小数据集
    - 数据不平衡效果不好
    - 必须要做数据标准化
    - 不适合特征维度太多的数据
  - 关于K的选取:K值的选取会影响到模型的效果
    - K越小,越容易过拟合
    - K越大，越容易欠拟合
    - 一般把K从1开始，依次增大
    - 


- 决策树

  - 决策树使用==信息增益==的方法来衡量一个特征和特征之间的重要性
  - 优点：

    - 非常直观，可解释性强
    - 预测速度比较快
    - 可以处理离散值，连续值，缺失值

  - 缺点：

    - 容易过拟合
    - 需要处理样本不均衡问题
    - 样本的变化会引发树结构的巨变

  - 解决办法：

    - 预剪枝
    - 后剪枝
- 随机森林

  - 使用bagging方案构建了多棵决策树，然后对所有树的结果来进行平均计算以获取最终的结果
  - GBDT构建的多棵树之间是有联系的，每个分类器是在上一轮分类器的残差基础上进行训练
  - XGBoost 优化了GBDT 里面求解的过程，并加入了很多工程上的优化项目
- 朴素贝叶斯

  - 优点：逻辑清晰简单，易于实现，适合大规模数据
    - 运算开销小
    - 预测过程快
    - 对于噪声和无关属性比较健壮
  - 缺点：具体应用时要考虑特征之间的相互独立性，再决定是否要使用该算法
  - 改进：
    - 半朴素贝叶斯ODE：建立一些属性间的联系，假定属性有一定的相关性，从而产生的算法
    - AODE：在ODE的基础上，使用bagging集成学习的思路，训练多个模型
- 支持向量机（SVM）

  - 假设找到一条线可以分隔红豆和绿豆
  - 红豆中距离这条线最近的几个样本点被称为{%  label  支持向量  blue  %}
  - 这些点到这条线的距离称为{%  label  间隔  orange  %}
  - 在决定最佳超平面时{% label 只有 red %}支持向量起作用，其他数据点并不起作用
  - 软间隔：在间隔区域允许出现一定数量的样本
  - 硬间隔：间隔区域没有样本
  - 非线性可分处理办法：{% label 把不可划分的样本映射到高维空间中 blue %}

    - 借助{% label 核函数 red %}来实现映射到高维的操作

      - 线性核函数
      - 多项式核函数
      - 高斯核函数

  - 优点：

    - 有严格的数学理论支持，可解释性强
    - 鲁棒性很好
    - 得到全局最优解

  - 缺点：

    - 训练所需的资源很大
    - 只能处理二分类问题
    - 模型预测时，预测时间与支持向量的个数成正比

- 人工神经网络：当前火热的深度学习基础

  - 算法原理：

    - 预先设定一种网络结构和激活函数
    - 初始化模型中的权重
    - 根据输入数据和权重来预测结果
    - 根据结果调节权重

  - 激活函数：

    - ReLU
    - tanh
    - Sigmoid

  - 优点：

    - 可以像搭积木一样不断地扩展模型的边界，而对于内部具体的运行不需要加以太多的干涉

  - 缺点：

    - 神经网络缺乏可解释性，内部纷繁复杂
    - 神经网络非常消耗资源

  - 深度学习：

    - 图像处理方面
    - 自然语言处理




#### (1) 二分类

要回答的问题只有"是"或"否"

#### (2) 多分类

在二分类的基础上,将标签可选范围扩大

#### (3) 多标签分类

多标签分类的下一条数据可以被标注上多个标签

### 4. 聚类问题

==聚类是无监督的学习过程==

- **互斥**:小组和小组之间是没有交集的,一个用户只存在于一个小组中
- **相交**:小组和小组之间有交集,一条数据可能既存在于A组,也存在于B组
- **层次:**一个大组还可以细分成若干个小组,比如高消费用户还可以分为累计高消费和单次高消费
- **模糊:**一个用户并不绝对属于某个小组,只能用概率表示他和某个小组的关系

方法:

- 基于划分的聚类
- 基于密度的聚类
- 基于层级的聚类
- 基于模型的聚类

#### (1) K-Means算法

- 先随机在空间中选取3个点，称之为中心点
- 计算所有的点到这三个点的距离，这里的距离使用欧氏距离
- 使用每个组的数据计算出这些数据的一个均值，使用这个均值作为下一轮迭代的中心点

如何确定K值：

- 手肘法： 循环尝试K值，计算在不同的K值情况下，所有数据的损失，即用每一个数据点到中心点的距离之和计算平均距离

优点：

- 简洁明了，计算复杂度低
- 收敛速度较快

缺点：

- 结果不稳定
- 无法解决样本不均衡的问题
- 容易收敛到局部最优解

#### (2) DBScan算法

**算法原理：**

- 直接密度可达：一个点在核心对象的半径区域内，称这个点和核心对象为直接密度可达
- 密度可达： 如果有一个系列的点，都满足上一个点到这个点是密度直达，那么这个系列中不相邻的点称为密度可达
- 密度相连： 如果从一个核心对象出发，得到两个密度可达的点，那么这两个点称为密度相连

**优点：**

- 不需要划分个数
- 可以处理噪声点
- 可以处理任意形状的空间聚类问题

**缺点：**

- 需要指定最小样本量和半径两个参数
- 数据量大时开销也很大
- 如果样本集的密度不均匀，聚类间距相差很大时，聚类质量较差

**动手实现**

**执行过程：**

### 5. 回归问题

**回归Regression(消退,回复)**

|      | 分类         | 回归         |
| ---- | ------------ | ------------ |
| 输出 | 离散数据     | 连续数据     |
| 目的 | 寻找决策边界 | 找到最优拟合 |

### 6. 关联问题

==关联无监督学习==

### 7. 模型集成

合成多个模型来提升整体的效果

方式

- Bagging(装袋法)
- Boosting(增强法)
- Stacking(堆叠法)

## (五) 评估模型(Evaluation)

### 1. 评估指标

#### (1) 混淆矩阵与准确率指标

**准确率相关指标**:可以直接反映一个模型对于样本数据的学习情况,是一种标准化检验

| 样本1000份  | 模型检测:是 | 模型检测:否 |
| ----------- | ----------- | ----------- |
| 人工标注:是 | 745(TP)     | 55(FN)      |
| 人工标注:否 | 25(FP)      | 175(TN)     |

混淆矩阵中包含四种数值

- 真阳性(True Positive, TP):小猪图被判定为小猪图
- FP真阴性(True Negative, TN): 不是小猪图被判定为不是小猪图
- 假阳性(False Positive, FP):不是小猪图被判定为小猪图
- 假阴性(False Negative, FN): 小猪图被判定为不是小猪图

**准确率(Accuracy):**所有预测正确的占全部样本的概率

$公式:Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$

**精确率(Precision):**预测正确的结果占所有预测成"是"的概率

$公式:Accuracy=\frac{TP}{TP+FP}$

**召回率(Recall):**该类别下预测正确的结果占该类别所有数据的概率

$公式:Accuracy=\frac{TP}{TP+FN}$

**F值(F Score):**准确率和召回率的调和平均值

$公式: 2*\frac{Accuracy*Recall}{Accuracy+Recall}$

**ROC曲线和AUC曲线**:构建了很多组混淆矩阵

真正利率: TP/(TP+FN)

假正利率: FP/(FP+TN)

#### (2). 业务指标

#### (3). 泛化能力

- 过拟合:训练集上表现良好,测试集或者验证集表现不佳
- 欠拟合: 在训练集和测试集上的表现都不好

#### (4). 模型速度

#### (5). 鲁棒性

#### (6). 可解释性

### 2.评估数据的处理

- 随机抽样: 把数据分成训练集和测试集,使用测试集对模型进行测试,得到各种准确率指标
- 随机多次抽样: 在随机抽样的基础上,进行n次随机抽样,得到n组测试集,使用这n组的平均值作为最终结果
- 交叉验证:需要选择多个模型,把原始的数据分为k份,每次选取其中的一份作为测试集,其他的作为训练集训练一个模型,计算这k个模型结果作为整体获得的准确率
- 自助法:随机有放回地抽取样本,构建一个训练集,对比原始样本集和该训练集,把训练集中未出现的内容整理成为测试集,重复这个过程k次,构建k个模型

## (六) 模型部署(Deployment)

# 五、实例

## (一) XGB实现酒店信息消歧

>理解数据
>
>- 数据属性不同
>- 数据形态不同
>- 数据量大，全部对比不现实
>- 消歧错误带来的分险



